# -*- coding: utf-8 -*-
"""test_ts_ticketing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S281827qCdv_R_peihlt1EVlsYARXJh_
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # unsloth for faster fine-tuning
# !pip install unsloth
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
# !pip install --upgrade transformers
# !pip install langchain_community

import pandas as pd
import json
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from sklearn.metrics import f1_score, precision_score, recall_score

hf_model = "kmcs-casulit/ts_ticket_v1.0.0.5"
token=os.environ['HUGGINGFACE_TOKEN']

tokenizer = AutoTokenizer.from_pretrained(
    hf_model,
    token=token    )

model = AutoModelForCausalLM.from_pretrained(
    hf_model,
    token=token,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=False
    ).to("cuda")


ticket_prompt = """
<Task_Context>
You are an expert at tagging tickets with their correct properties.

You will be given with:
Ticket information in JSON format (fields: subject, description, email).
A list of the possible ticket property values in JSON format (fields: department, techgroup, category, subcategory, priority).

Assign the most appropriate value for each property, using only the provided ticket information and the possible property values.
If you are unsure or the information is insufficient, set the property to null.
Do NOT invent or guess values outside the provided options.

<VERY IMPORTANT>
Return ONLY the JSON object for the ticket properties. Do NOT include any explanations, extra text, or formatting such as markdown or code blocks.
</VERY IMPORTANT>
</Task_Context>

<Examples>
Example 1:
<Ticket_Information>
{{"subject": "UPT20 (SO-NickScali) : PC assistance Chanel Tolentino", "description": "UPT20 (SO-NickScali) : PC assistance Chanel Tolentino", "email": "chanel.tolentino@company.com"}}
</Ticket_Information>
<Possible_Output>
{{"department": "Technology Services", "techgroup": "On-Site Support", "category": "Hardware", "subcategory": "Desktop/Laptop Problem", "priority": "P2 - General"}}
</Possible_Output>

Example 2:
<Ticket_Information>
{{"subject": "PODIUM | VITRO LINK | Alert OSPF Neighbor is Down - VITRO MAKATI PODIUM-S2", "description": "PODIUM | VITRO LINK | Alert OSPF Neighbor is Down - VITRO MAKATI PODIUM-S2", "email": "noc.alerts@company.com"}}
</Ticket_Information>
<Possible_Output>
{{"department": "Technology Services", "techgroup": "NOC", "category": "Outage", "subcategory": "ISP Outage", "priority": "P1 - Critical"}}
</Possible_Output>
</Examples>

<Ticket_Information>
{}
</Ticket_Information>
{}
"""

def calculate_metric_for_keys(metric_func, ground_truth, prediction):
    scores = {}
    for key in ground_truth[0]:  # Assuming both dictionaries have the same keys
        gt_values = [d[key] for d in ground_truth]
        pred_values = [d[key] for d in prediction]
        score = metric_func(gt_values, pred_values, average='weighted', zero_division=0)
        scores[key] = score
    return scores

def process_row(row):

    inputs = tokenizer(
        [
            ticket_prompt.format(row["ticket_information"], "")
        ],
        return_tensors="pt",
    ).to("cuda")

    # Generate the response
    model_response = model.generate(**inputs, max_new_tokens = 2048)

    # Decode the response to get the text
    generated_text = tokenizer.decode(model_response[0], skip_special_tokens=True)

    # Find the start and end of the Output_Properties section
    start_index = generated_text.find("<Output_Properties>") + len("<Output_Properties>")
    end_index = generated_text.find("</Output_Properties>")

    # Extract the content within the Output_Properties tags
    generated_response = generated_text[start_index:end_index].strip()

    try:
        generated_json = json.loads(generated_response)
    except json.JSONDecodeError:
        generated_json = {}

    # Store generated properties as JSON string
    row["ticket_properties_OUTPUT"] = json.dumps(generated_json)

        # Calculate metrics for the current row
    ground_truth = [json.loads(row["ticket_properties"])]
    prediction = [generated_json]

    # Calculate and store metrics
    accuracy = 1 if ground_truth == prediction else 0  # Calculate accuracy

    metrics = {
        "f1_scores": calculate_metric_for_keys(f1_score, ground_truth, prediction),
        "precision_scores": calculate_metric_for_keys(precision_score, ground_truth, prediction),
        "recall_scores": calculate_metric_for_keys(recall_score, ground_truth, prediction),
        "accuracy": accuracy  # Add accuracy to the metrics dictionary
    }

    # Store metrics as JSON string
    row["metrics"] = json.dumps(metrics)

    # Store generated properties as JSON string
    row["ticket_properties_OUTPUT"] = json.dumps(generated_json)

    return row

#Load the CSV file
test_df = pd.read_csv("ts_ticketing_test_results_v1.0.0.5.csv")

# Apply the process_row function to each row
test_df = test_df.apply(process_row, axis=1)

# Save the updated CSV file
test_df.to_csv("ts_ticketing_test_results_v1.0.0.5.csv", index=False)

# #Load the CSV file
# test_df = pd.read_csv("ts_ticketing_test_results_v1.0.0.5.csv")

# # Apply the process_row function to each row
# test_df = test_df.apply(process_row, axis=1)

# # Save the updated CSV file
# test_df.to_csv("ts_ticketing_test_results_v1.0.0.5.csv", index=False)

# #if True: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit")
# if True: merged_model.push_to_hub("kmcs-casulit/ts_ticket_v1.0.0.3", token = "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")
# if True: tokenizer.push_to_hub("kmcs-casulit/ts_ticket_v1.0.0.3", token = "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")

test_df['accuracy_value'] = test_df['metrics'].apply(lambda x: json.loads(x)['accuracy'])
accuracy_percentage = test_df['accuracy_value'][:100].mean() * 100

test_df['f1_values'] = test_df['metrics'].apply(lambda x: json.loads(x)['f1_scores'])
f1_department = test_df['f1_values'][:100].apply(lambda x: x['department']).mean() * 100
f1_techgroup = test_df['f1_values'][:100].apply(lambda x: x['techgroup']).mean() * 100
f1_category = test_df['f1_values'][:100].apply(lambda x: x['category']).mean() * 100
f1_subcategory = test_df['f1_values'][:100].apply(lambda x: x['subcategory']).mean() * 100
f1_priority = test_df['f1_values'][:100].apply(lambda x: x['priority']).mean() * 100


test_df['precision_values'] = test_df['metrics'].apply(lambda x: json.loads(x)['precision_scores'])
precision_department = test_df['precision_values'][:100].apply(lambda x: x['department']).mean() * 100
precision_techgroup = test_df['precision_values'][:100].apply(lambda x: x['techgroup']).mean() * 100
precision_category = test_df['precision_values'][:100].apply(lambda x: x['category']).mean() * 100
precision_subcategory = test_df['precision_values'][:100].apply(lambda x: x['subcategory']).mean() * 100
precision_priority = test_df['precision_values'][:100].apply(lambda x: x['priority']).mean() * 100

test_df['recall_values'] = test_df['metrics'].apply(lambda x: json.loads(x)['recall_scores'])
recall_department = test_df['recall_values'][:100].apply(lambda x: x['department']).mean() * 100
recall_techgroup = test_df['recall_values'][:100].apply(lambda x: x['techgroup']).mean() * 100
recall_category = test_df['recall_values'][:100].apply(lambda x: x['category']).mean() * 100
recall_subcategory = test_df['recall_values'][:100].apply(lambda x: x['subcategory']).mean() * 100
recall_priority = test_df['recall_values'][:100].apply(lambda x: x['priority']).mean() * 100

print("------------------------------------------------------------------------")
print(f"Accuracy Percentage for 100 Test Cases: {accuracy_percentage:.2f}%")
print("------------------------------------------------------------------------")
print(f"F1 Percentage for Department for 100 Test Cases: {f1_department:.2f}%")
print(f"F1 Percentage for Techgroup for 100 Test Cases: {f1_techgroup:.2f}%")
print(f"F1 Percentage for Category for 100 Test Cases: {f1_category:.2f}%")
print(f"F1 Percentage for Subcategory for 100 Test Cases: {f1_subcategory:.2f}%")
print(f"F1 Percentage for Priority for 100 Test Cases: {f1_priority:.2f}%")
print("------------------------------------------------------------------------")
print(f"Precision Percentage for Department for 100 Test Cases: {precision_department:.2f}%")
print(f"Precision Percentage for Techgroup for 100 Test Cases: {precision_techgroup:.2f}%")
print(f"Precision Percentage for Category for 100 Test Cases: {precision_category:.2f}%")
print(f"Precision Percentage for Subcategory for 100 Test Cases: {precision_subcategory:.2f}%")
print(f"Precision Percentage for Priority for 100 Test Cases: {precision_priority:.2f}%")
print("------------------------------------------------------------------------")
print(f"Recall Percentage for Department for 100 Test Cases: {recall_department:.2f}%")
print(f"Recall Percentage for Techgroup for 100 Test Cases: {recall_techgroup:.2f}%")
print(f"Recall Percentage for Category for 100 Test Cases: {recall_category:.2f}%")
print(f"Recall Percentage for Subcategory for 100 Test Cases: {recall_subcategory:.2f}%")
print(f"Recall Percentage for Priority for 100 Test Cases: {recall_priority:.2f}%")
print("------------------------------------------------------------------------")
